### **Jorge Mario Trejos Barquero.**
### **B77676.**

#### **Tutorial 1**
Para este primer tutorial se seleccionó el tutorial llamado "How to Develop a Random Forest Ensemble in Python".

De este primer tutorial se aprendió que los random forest sirven para predicciones de clasificaciones o regresión, además de la importancia de los hiperparametros tales como profundidad, máximo de arboles, etc; los cuales hacen variar en la precisión y capacidad de explicar un fenómeno mediante un random forest.

Cabe destacar que de todos estos parámetros surge la duda de como puedo yo saber a priori cuales valores para estos parámetros son óptimos. La respuesta de esta pregunta no la sé y me gustaría que fuera explicada en clase, pero si sé que para el parámetro de features, la cantidad de features recomendada normalmente es la raíz cuadrada de la cantidad total de features que haya en el dataset.

#### **Tutorial 2**
Para este segundo tutorial se seleccionó el tutorial llamado "How to Develop an Extra Trees Ensemble with Python".

La principal diferencia de los random forest y árboles adicionales es que los random forest desarrollan cada árbol de decisión a partir de una muestra de arranque del conjunto de datos de entrenamiento, el algoritmo árboles adicionales ajusta cada árbol de decisión en todo el conjunto de datos de entrenamiento.

De este segundo tutorial no hay mucha diferencia respecto al primero ya que al igual que en el anterior lo que se hace es explorar el impacto de los hiperparametros en los árboles adicionales. Lo único diferente es que se explora un parámetro que no se vio en el caso de los random forest y es el de minimum samples per split.

Las dudas que surgen de este segundo tutorial son las mismas que existían del primero, ¿Como se puede elegir un valor óptimo para estos hiperparametros? De igual manera que en el tutorial anterior se dijo, se espera que estas respuesta sean dadas en clases.