### **Jorge Mario Trejos Barquero.**
### **B77676.**

#### **Resumen**
En estas 2 semanas se retomó el tema del análisis exploratorio empezando con la mención de la importancia de realizar inspección visual a los datos mediante gráficas, plots, etc. Luego se mencionaron los tipos de datos que son numéricos y categóricos. A continuación se mencionaron las características principales de un dataset que son:

1. Dimensionalidad: El numero de columnas, atributos, características, etc.
2. Dispersión: Significa que hay datos más representativos que otros.
3. Resolución: Que tan precisos son los datos que tenemos.

Luego se habló sobre las formas de trabajar un dataset que son:

1. Agregación: Combinar atributos para un mejor entendimiento del dataset.
2. Muestreo: Trabajar un subconjunto representativo de todo el dataset, este puede ser: Aleatorio con reemplazo, sin reemplazo, estratificado, progresivo, etc.
3. Reducción de la dimensionalidad: Eliminar atributos de ruido y no informativos.
4. Selección de atributos: El principio es el mismo que el del anterior punto, pero en esta forma lo que se hace más que eliminarlos es combinarlos en ciertos casos, se usan técnicas de Weighting, embedded, filter y wrapper.
5. creación de atributos: Crear nuevos atributos significativos y valiosos a partir de otros ya existentes.
6. Binarización y discretización.
7. Transformación de variables: Normalización, estandarización, etc.

Se cambió de tema y ahora se hablo sobre algoritmos de machine learning. Existen tipos como: Clasificación, regresión y segmentación. Y modelos como: Regresión lineal y arboles de decisión.

Modelo de ML = datos + algoritmo de predicciones

Ahora se habló de como construir modelos. Un modelo lo que trata es representar un fenómeno de la vida real y buscar patrones en este para su estudio, existen definiciones como las de overfitting y underfitting que significan adaptar un algoritmo de ML mucho a un dataset entonces no reconoce nada que no sea ese dataset (overfitting) o el caso contrario donde no reconoce nada porque le faltó entrenamiento (underfitting).
No siempre tener un gran dataset con muchos atributos significa que va a explicar mejor un fenómeno, esto ya que se pueden haber variables de ruido y confusión. Existen modelos de tipo:

1. Lineal vs no lineal.
2. Caja negra vs caja blanca.
3. first principle vs data driven.
4. estocásticos vs deterministas.
5. planos vs jerárquicos.

El dataset tiene que estar dividido en 3 partes, una para realizar el training, otra para la validación y una final con la que realizar los test.
Los binary classifiers pueden ser: true positive, true negative, false positive y false negative.
Finalmente se habló de arboles de decisión, donde se mencionó que estos pueden aprender por medio de la regresión o clasificación. Este tema de arboles de decisión solo se introdujo un poco pero no se a hablado de él a profundidad aún.

#### **Aprendido**
Podríamos dividir todo lo que se vio en clase en 2 temas: análisis exploratorio (EDA) y machine learning (ML); Todo lo que sea referente a ML es nuevo para mi ya que soy del énfasis de TI y en este énfasis no se ve ML, pero refieren al EDA si sabía bastante gracias al curso de diseño de experimentos.

#### **Dudas**
Me gustaría entrar en más detalle de como funcionan los algoritmos de ML ya que para mi de momento son algoritmos de caja negra ya que, sé que resultados devuelven, pero no sé como hacen para calcular estos resultados.

#### **Uso en el futuro**
El EDA tiene un gran uso a nivel de redes para estudio de su funcionamiento. Para un TI el ML no es tan importante pero me parece súper interesante y si tuviera que trabajar con ML en un futuro lo haría ya que es un tema que me interesa.

#### **Material extra**
No se consultó ningún material extra, solo se buscó por google definiciones de palabras, traducciones, funciones para tareas, pero como tal algo en concreto no se consultó. Se visitó múltiples veces la documentación de scikit para saber como funcionaban sus funciones.
https://scikit-learn.org/stable/
